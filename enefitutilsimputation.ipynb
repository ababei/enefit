{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook, we have utilities for imputing missing data in the Enefit time series using MICE https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html\n\nCitation:\nKristjan Eljand, Martin Laid, Jean-Baptiste Scellier, Sohier Dane, Maggie Demkin, Addison Howard. (2023). \nEnefit - Predict Energy Behavior of Prosumers. Kaggle. https://kaggle.com/competitions/predict-energy-behavior-of-prosumers\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport random\n\nfrom datetime import datetime, timedelta\nfrom sklearn.experimental import enable_iterative_imputer \nfrom sklearn.impute import IterativeImputer\nfrom functools import reduce\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-15T23:45:38.129863Z","iopub.execute_input":"2024-02-15T23:45:38.130330Z","iopub.status.idle":"2024-02-15T23:45:39.339792Z","shell.execute_reply.started":"2024-02-15T23:45:38.130294Z","shell.execute_reply":"2024-02-15T23:45:39.338480Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# 1. Separating the time series\n\nThe Enefit training set consists of separate time series, each of them corresponding to a quadruple (county_value, is_business_value, is_consumption_value, product_type_value). We call such quadruples the signature of a time series. In the following code, we define a class that separates the training set into the different time series, each of them corresponding to a column in a new dataframe.","metadata":{}},{"cell_type":"code","source":"class SignedTimeSeries:\n    \"\"\"\n    Creates a dataframe indexed by the datetime when a reading is take, and whose columns are the target values in each time series. Signed means that we assign each column to a signature.\n    \"\"\"\n    \n    def __init__(self, df):\n        self.df = df\n        self.data = self.separate()\n\n    def separate(self):\n        signed_series=[]\n        for group, group_df in df.groupby(['county', 'is_business', 'is_consumption', 'product_type']):\n            signature=(group_df['county'].iloc[0], group_df['is_business'].iloc[0], group_df['is_consumption'].iloc[0], group_df['product_type'].iloc[0])\n            group_df = group_df.set_index('datetime')\n            group_df=group_df[['target']].copy()\n            group_df.rename(columns={'target': 'target' +'_' +str(int(signature[0]))+'_'+str(int(signature[1]))+'_'+ str(int(signature[2]))+'_'+ str(int(signature[3]))}, inplace=True)\n            signed_series.append(group_df)\n        separated_series = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='outer'), signed_series)   \n        return separated_series\n        \nsts = SignedTimeSeries(df)","metadata":{"execution":{"iopub.status.busy":"2024-02-15T23:45:39.342237Z","iopub.execute_input":"2024-02-15T23:45:39.342687Z","iopub.status.idle":"2024-02-15T23:45:40.318621Z","shell.execute_reply.started":"2024-02-15T23:45:39.342664Z","shell.execute_reply":"2024-02-15T23:45:40.317769Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# 2. Impute time series\n\nThe SignedTimeSeries defined above has NaN values. We now impute those values, starting from the series that need less data imputed and moving on to the series that need more. We impute in series with signature (c, ib, ic, pt) by using series with signature (x, ib, ic, pt), where x ranges through all available county values. This is because such series are highly correlated.","metadata":{}},{"cell_type":"code","source":"# Global function for imputing any column in any data\ndef impute(df, column, scale = True): # set all_corr = True to impute using ALL series that are highly correlated (>0.9) with the one we want to impute, not just those sharing the same\n                                          # is_business, is_consumption, and product_type value.\n    if scale == True:   # we first scale the columns\n        max_vals={columns: df[columns].max() for columns in df.columns}\n    df=df.div(max_vals, axis=1)\n    # Impute values\n    imputer = IterativeImputer()\n    imputed_data = pd.DataFrame(imputer.fit_transform(df), columns=df.columns, index=df.index)\n    if scale ==  True: # scale back\n        imputed_data= imputed_data.multiply(max_vals, axis=1)\n    return imputed_data\n        \n\n    \nclass ImputedTimeSeries(SignedTimeSeries):\n    \"\"\"\n    class that imputes the missing values and calculates the error on a random 28-day test set\n    \"\"\"\n    \n    def nan_signatures(self): #returns a dictionary whose keys are the column names that contain NaN values, and values are the lengths of the data that needs to be imputed\n        values={}\n        for column in self.data.columns:\n            nan_count = self.data[column].isna().sum()\n            if nan_count > 0:\n                values[column]=nan_count\n        return values\n\n    \n    \n    def impute(self, column, all_corr = False, scale = True): # set all_corr = True to impute using ALL series that are highly correlated (>0.9) with the one we want to impute, not just those sharing the same\n                                          # is_business, is_consumption, and product_type value.\n        # get relevant columns\n        if all_corr == False:\n            relevant_columns=[columns for columns in self.data.columns if columns[-5:]==column[-5:]]\n        else:\n            \n        new_data = self.data[relevant_columns]\n        imputed_data=impute(new_data, column, scale = scale)\n        self.data[column] = imputed_data[column]\n    \n    \n    def calculate_error(self, column, all_corr = False, scale = True, visualize = False): #plots and calculates error on a random subset of 4 weeks' data \n        # set visualize to True to plot\n        # get the month range\n        non_nan_periods = self.data[self.data[column].notna()].resample('28D').mean().dropna().index\n        ind = random.randint(0, len(non_nan_periods) - 1); \n        start_date = non_nan_periods[ind]; end_date = start_date + timedelta(days=28)\n        range_start = self.data.index.get_loc(start_date); range_end = range_start+28*24;\n        # replace some values by NaN\n        # get relevant columns\n        if all_corr == False:\n            relevant_columns=[columns for columns in self.data.columns if columns[-5:]==column[-5:]]\n        else:\n            \n        test_data = self.data[relevant_columns].copy()\n        test_data.loc[start_date : end_date, column] = np.nan\n        imputed_data=impute(test_data, column, scale = scale)\n        # plot range\n        if visualize == True:\n            plt.figure(figsize=(18, 8))\n            plt.plot(self.data.index.to_list()[range_start:range_end], self.data[column].to_list()[range_start:range_end], label='Original')\n            plt.plot(self.data.index.to_list()[range_start:range_end], imputed_data[column].to_list()[range_start:range_end], label='Imputed data', linestyle='--')\n            plt.title('Comparison of Original vs. Imputed Time Series with Shaded Area')\n            plt.xlabel('Datetime')\n            plt.ylabel('Values')\n            plt.legend()\n        # calculate error\n        return tf.keras.metrics.mean_absolute_error(self.data[column].to_list()[range_start:range_end], imputed_data[column].to_list()[range_start:range_end]).numpy()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-16T00:44:23.480847Z","iopub.execute_input":"2024-02-16T00:44:23.481325Z","iopub.status.idle":"2024-02-16T00:44:24.902397Z","shell.execute_reply.started":"2024-02-16T00:44:23.481290Z","shell.execute_reply":"2024-02-16T00:44:24.900973Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"        \nsts = SignedTimeSeries(df)\nsts1 = ImputedTimeSeries(sts)","metadata":{"execution":{"iopub.status.busy":"2024-02-16T00:50:02.933749Z","iopub.execute_input":"2024-02-16T00:50:02.934227Z","iopub.status.idle":"2024-02-16T00:50:05.163015Z","shell.execute_reply.started":"2024-02-16T00:50:02.934188Z","shell.execute_reply":"2024-02-16T00:50:05.160832Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"sts1.nan_signatures()","metadata":{"execution":{"iopub.status.busy":"2024-02-16T00:50:14.126651Z","iopub.execute_input":"2024-02-16T00:50:14.127245Z","iopub.status.idle":"2024-02-16T00:50:14.155029Z","shell.execute_reply.started":"2024-02-16T00:50:14.127217Z","shell.execute_reply":"2024-02-16T00:50:14.153853Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"{'target_0_1_0_2': 720,\n 'target_0_1_1_2': 720,\n 'target_2_0_0_1': 720,\n 'target_2_0_1_1': 720,\n 'target_2_1_0_1': 2928,\n 'target_2_1_1_1': 2928,\n 'target_4_1_0_1': 2928,\n 'target_4_1_1_1': 2928,\n 'target_11_1_0_0': 3672,\n 'target_11_1_1_0': 3672,\n 'target_13_1_0_1': 720,\n 'target_13_1_1_1': 720,\n 'target_15_1_0_0': 2184,\n 'target_15_1_1_0': 2184}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}