{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a67d035",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.003811,
     "end_time": "2024-02-16T20:46:09.036216",
     "exception": false,
     "start_time": "2024-02-16T20:46:09.032405",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this notebook, we have utilities for imputing missing data in the Enefit time series using MICE https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html\n",
    "\n",
    "Citation:\n",
    "Kristjan Eljand, Martin Laid, Jean-Baptiste Scellier, Sohier Dane, Maggie Demkin, Addison Howard. (2023). \n",
    "Enefit - Predict Energy Behavior of Prosumers. Kaggle. https://kaggle.com/competitions/predict-energy-behavior-of-prosumers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "577bc23e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T20:46:09.045927Z",
     "iopub.status.busy": "2024-02-16T20:46:09.044716Z",
     "iopub.status.idle": "2024-02-16T20:46:30.233160Z",
     "shell.execute_reply": "2024-02-16T20:46:30.231651Z"
    },
    "papermill": {
     "duration": 21.196904,
     "end_time": "2024-02-16T20:46:30.236843",
     "exception": false,
     "start_time": "2024-02-16T20:46:09.039939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 20:46:12.900213: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-16 20:46:12.900354: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-16 20:46:13.093523: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from functools import reduce\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8e3eca",
   "metadata": {
    "papermill": {
     "duration": 0.00335,
     "end_time": "2024-02-16T20:46:30.244091",
     "exception": false,
     "start_time": "2024-02-16T20:46:30.240741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Separating the time series\n",
    "\n",
    "The Enefit training set consists of separate time series, each of them corresponding to a quadruple (county_value, is_business_value, is_consumption_value, product_type_value). We call such quadruples the signature of a time series. In the following code, we define a class that separates the training set into the different time series, each of them corresponding to a column in a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "377d5ded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T20:46:30.254887Z",
     "iopub.status.busy": "2024-02-16T20:46:30.254087Z",
     "iopub.status.idle": "2024-02-16T20:46:30.266559Z",
     "shell.execute_reply": "2024-02-16T20:46:30.265169Z"
    },
    "papermill": {
     "duration": 0.020838,
     "end_time": "2024-02-16T20:46:30.269254",
     "exception": false,
     "start_time": "2024-02-16T20:46:30.248416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SignedTimeSeries:\n",
    "    \"\"\"\n",
    "    Creates a dataframe indexed by the datetime when a reading is take, and whose columns are the target values in each time series. Signed means that we assign each column to a signature.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.separate()\n",
    "\n",
    "    def separate(self):\n",
    "        signed_series=[]\n",
    "        df=self.df\n",
    "        for group, group_df in df.groupby(['county', 'is_business', 'is_consumption', 'product_type']):\n",
    "            signature=(group_df['county'].iloc[0], group_df['is_business'].iloc[0], group_df['is_consumption'].iloc[0], group_df['product_type'].iloc[0])\n",
    "            group_df = group_df.set_index('datetime')\n",
    "            group_df=group_df[['target']].copy()\n",
    "            group_df.rename(columns={'target': 'target' +'_' +str(int(signature[0]))+'_'+str(int(signature[1]))+'_'+ str(int(signature[2]))+'_'+ str(int(signature[3]))}, inplace=True)\n",
    "            signed_series.append(group_df)\n",
    "        separated_series = reduce(lambda left, right: pd.merge(left, right, left_index=True, right_index=True, how='outer'), signed_series)   \n",
    "        self.data = separated_series\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58b23b5",
   "metadata": {
    "papermill": {
     "duration": 0.003233,
     "end_time": "2024-02-16T20:46:30.276173",
     "exception": false,
     "start_time": "2024-02-16T20:46:30.272940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Impute time series\n",
    "\n",
    "The SignedTimeSeries defined above has NaN values. We now impute those values, starting from the series that need less data imputed and moving on to the series that need more. We impute in series with signature (c, ib, ic, pt) by using series with signature (x, ib, ic, pt), where x ranges through all available county values. This is because such series are highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d286fab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T20:46:30.286768Z",
     "iopub.status.busy": "2024-02-16T20:46:30.286324Z",
     "iopub.status.idle": "2024-02-16T20:46:30.312658Z",
     "shell.execute_reply": "2024-02-16T20:46:30.311094Z"
    },
    "papermill": {
     "duration": 0.035073,
     "end_time": "2024-02-16T20:46:30.315657",
     "exception": false,
     "start_time": "2024-02-16T20:46:30.280584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Global function for imputing any column in any data\n",
    "def impute(df, column, scale = True): # set all_corr = True to impute using ALL series that are highly correlated (>0.9) with the one we want to impute, not just those sharing the same\n",
    "                                          # is_business, is_consumption, and product_type value.\n",
    "    if scale == True:   # we first scale the columns\n",
    "        #max_vals={columns: df[columns].max() for columns in df.columns}\n",
    "        max_vals = df.max(axis=0)\n",
    "        df=df.div(max_vals, axis=1)\n",
    "    # Impute values\n",
    "    imputer = IterativeImputer()\n",
    "    imputed_data = pd.DataFrame(imputer.fit_transform(df), columns=df.columns, index=df.index)\n",
    "    if scale ==  True: # scale back\n",
    "        imputed_data= imputed_data.multiply(max_vals, axis=1)\n",
    "    return imputed_data\n",
    "        \n",
    "\n",
    "    \n",
    "class ImputedTimeSeries(SignedTimeSeries):\n",
    "    \"\"\"\n",
    "    class that imputes the missing values and calculates the error on a random 28-day test set\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, signed_time_series):\n",
    "        super().__init__(signed_time_series.df)\n",
    "        self.separate()  # Call the separate method to update self.data\n",
    "\n",
    "\n",
    "    def nan_signatures(self): #returns a dictionary whose keys are the column names that contain NaN values, and values are the lengths of the data that needs to be imputed\n",
    "        values={}\n",
    "        for column in self.data.columns:\n",
    "            nan_count = self.data[column].isna().sum()\n",
    "            if nan_count > 0:\n",
    "                values[column]=nan_count\n",
    "        return values\n",
    "\n",
    "    \n",
    "    \n",
    "    def impute(self, column, all_corr = False, scale = True): # set all_corr = True to impute using ALL series that are highly correlated (>0.9) with the one we want to impute, not just those sharing the same\n",
    "                                          # is_business, is_consumption, and product_type value.\n",
    "        # get relevant columns\n",
    "        if all_corr == False:\n",
    "            relevant_columns=[columns for columns in self.data.columns if columns[-5:]==column[-5:]]\n",
    "        else:\n",
    "            relevant_columns = self.corr_signature(column).index.to_list()\n",
    "        new_data = self.data[relevant_columns]\n",
    "        imputed_data=impute(new_data, column, scale = scale)\n",
    "        self.data[column] = imputed_data[column]\n",
    "    \n",
    "    \n",
    "    def calculate_error(self, column, all_corr = False, scale = True, visualize = False): #plots and calculates error on a random subset of 4 weeks' data \n",
    "        # if all_corr = True, we impute using all series that are highly correlated with this column\n",
    "        # set visualize to True to plot\n",
    "        # get the month range\n",
    "        non_nan_periods = self.data[self.data[column].notna()].resample('28D').mean().dropna().index\n",
    "        ind = random.randint(0, len(non_nan_periods) - 1); \n",
    "        start_date = non_nan_periods[ind]; end_date = start_date + timedelta(days=28)\n",
    "        range_start = self.data.index.get_loc(start_date); range_end = range_start+28*24;\n",
    "        # replace some values by NaN\n",
    "        # get relevant columns\n",
    "        if all_corr == False:\n",
    "            relevant_columns=[columns for columns in self.data.columns if columns[-5:]==column[-5:]]\n",
    "        else:\n",
    "            relevant_columns = self.corr_signature(column).index.to_list()\n",
    "        #impute data\n",
    "        test_data = self.data[relevant_columns].copy()\n",
    "        test_data.loc[start_date : end_date, column] = np.nan\n",
    "        imputed_data=impute(test_data, column, scale = scale)\n",
    "        # plot range\n",
    "        if visualize == True:\n",
    "            plt.figure(figsize=(18, 8))\n",
    "            plt.plot(self.data.index[range_start:range_end], self.data[column][range_start:range_end], label='Original')\n",
    "            plt.plot(self.data.index[range_start:range_end], imputed_data[column][range_start:range_end], label='Imputed data', linestyle='--')\n",
    "            plt.title('Comparison of Original vs. Imputed Time Series for series ' + column)\n",
    "            plt.xlabel('Datetime')\n",
    "            plt.ylabel('Values')\n",
    "            plt.legend()\n",
    "        # calculate error\n",
    "        return tf.keras.metrics.mean_absolute_error(self.data[column].to_list()[range_start:range_end], imputed_data[column].to_list()[range_start:range_end]).numpy(), len(relevant_columns)-1\n",
    "\n",
    "\n",
    "    def corr_signature(self, column): #obtains the correlations of specific column with all other columns\n",
    "        nans= self.nan_signatures().keys()\n",
    "        full_columns = [column for column in self.data.columns if column not in nans]+[column]\n",
    "        correlations = self.data[full_columns].corr()[column]\n",
    "        return correlations[correlations > 0.9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43fdc16",
   "metadata": {
    "papermill": {
     "duration": 0.00477,
     "end_time": "2024-02-16T20:46:30.327514",
     "exception": false,
     "start_time": "2024-02-16T20:46:30.322744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27.331312,
   "end_time": "2024-02-16T20:46:32.757834",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-16T20:46:05.426522",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
